---
title: "Automated Bayesian Model Selector"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    theme: yeti
    social: menu
    source_code: embed
runtime: shiny
---


```{r setup, include=FALSE}
library(flexdashboard)
library(ggROC)
library(tidyposterior)
library(plyr)
library(caret)
library(randomForest)
library(tidymodels)
library(gbm)
library(DataExplorer)
library(tidyverse)
library(pROC)
library(e1071)

# loading data -------------------------------------------------------------------------------

churn_data <- read_csv("data/src/WA_Fn-UseC_-Telco-Customer-Churn.csv") %>% 
     filter(!is.na(TotalCharges)) %>%
     select(-customerID)



# data sampling ------------------------------------------------------------------------------

seed <- 7312
set.seed(seed)
data_split <- initial_split(churn_data, strata = "Churn")
churn_train <- training(data_split)
churn_test  <- testing(data_split) %>%
  mutate(Churn = as.factor(Churn))

trainX <- churn_train %>%
     select(-Churn)
trainY <- churn_train %>%
     select(Churn)

plot_roc <- function(x) {
        averaged <- x %>%
                group_by(rowIndex, obs) %>%
                summarise(Yes = mean(Yes, na.rm = TRUE))
        roc_obj <- roc(
                response = x[["obs"]], 
                predictor = x[["Yes"]], 
                levels = rev(levels(x$obs))
        )
        return(roc_obj)
}

```

Model Selection
===================================== 

Column {data-width=350}
-----------------------------------------------------------------------

### ROC Performance of all Models

```{r}

load("models/model_1.Rdata")

load("models/model_2.Rdata")

load("models/model_3.Rdata")

# model accuracy ----------------------------------------------------------------------------

roc_mod1 <- plot_roc(model_1$pred)
    roc_mod2 <- plot_roc(model_2$pred)
    roc_mod3 <- plot_roc(model_3$pred)
    ggroc(list(Model_1=roc_mod1, Model_2=roc_mod2, Model3=roc_mod3)) +
      theme(legend.position = "top")
```

### ROC results after 5000 simulations

```{r, include=FALSE}
rs <- resamples(
     list(Model1 = model_1, Model2 = model_2, Model3 = model_3)
  )
  roc_mod <- perf_mod(rs, seed = seed, iter = 5000)
  roc_dist <- tidy(roc_mod)
  temp <- summary(roc_dist)
```

```{r}
DT::datatable(temp)
```

### Distribution of all three models

```{r}
ggplot(roc_dist) +
      theme(legend.position = "top")
```


Column {data-width=650}
-----------------------------------------------------------------------


### Model Selection

```{r, include=FALSE}
differences <-
     contrast_models(
          roc_mod,
          list_1 = c("Model2", "Model3"),
          list_2 = c("Model1", "Model2"),
          seed = 650
     )

  pracDif <- 5/100
  dif <- summary(differences, size = pracDif)
```

```{r}
DT::datatable(dif)
```

### Visual Summary of Model Difference

```{r}
differences %>%
     mutate(contrast = paste(model_2, "vs", model_1)) %>%
     ggplot(aes(x = difference, col = contrast)) + 
     geom_line(stat = "density") + 
     geom_vline(xintercept = c(-pracDif, pracDif), lty = 2) +
     theme(legend.position = "top")
```


```{r, include=FALSE}
modSlc <- temp %>%
    filter(mean == max(mean))
  temp2 <- modSlc$model
  
if(temp2 == "Model1"){
    mod_is <- model_1
  } else if(temp2 == "Model2"){
    mod_is <- model_2
  } else {
    mod_is <- model_3
  }

  output$testRoc <- renderPlot({
    test_res <- churn_test %>%
      dplyr::select(Churn) %>%
      mutate(Churn = as_factor(Churn)) %>%
      mutate(
        pred = predict(mod_is, churn_test),
        prob = predict(mod_is, churn_test, type = "prob")[, "Yes"]
      ) %>%
      data.frame
    roc_curve <- roc(test_res$Churn, test_res$prob, levels = c("No", "Yes"))
    ggroc(roc_curve, alpha = 0.5, colour = "red", size = 1.5) +
      theme(legend.position = "top")               
  })

  churn_test$Churn <- as_factor(churn_test$Churn)
  prediction <- predict(mod_is, churn_test %>% dplyr::select(-Churn))
  cnfsn <- confusionMatrix(prediction, churn_test$Churn)
  ratespec <- round(as.numeric(cnfsn$byClass[2]),2)
  ratesens <- round(as.numeric(cnfsn$byClass[1]),2)
  
  output$testSpec <- renderGauge({
    gauge(ratespec, min = 0, max = 1, gaugeSectors(
      success = c(.85, 1), warning = c(.51, .84), danger = c(0, .50)
    ))
  })
  
  output$testSens <- renderGauge({
    gauge(ratesens, min = 0, max = 1, gaugeSectors(
      success = c(.85, 1), warning = c(.51, .84), danger = c(0, .50)
    ))
  })
  output$confM <- renderPrint({
    cnfsn
  })
```


Final Model Validation
=====================================

Column {data-width=250}
-----------------------------------------------------------------------

### Validation Sensitivity

```{r}
gaugeOutput("testSens")
```


### Validation Specificity

```{r}
gaugeOutput("testSpec")
```

### Confusion Matrix of validation set

```{r}
verbatimTextOutput("confM")
```

Column {data-width=650}
-----------------------------------------------------------------------

### Test data ROC curve

```{r}
plotOutput("testRoc")
```